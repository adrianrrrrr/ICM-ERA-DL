import time
import numpy as np
import os
import sys
import gc

from typing import Tuple, Dict, Any, List


import matplotlib
import matplotlib.pyplot as plt

import mlx as ml
import mlx.core as ml # Adding support for Apple Silicon
import mlx.nn.layers as nn_mlx

from netCDF4 import Dataset as netDataset
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, TensorDataset, random_split

# Check Apple Silicon availability
if not torch.backends.mps.is_available():
    if not torch.backends.mps.is_built():
        print("MPS not available because the current PyTorch install was not "
              "built with MPS enabled.")
    else:
        print("MPS not available because the current MacOS version is not 12.3+ "
              "and/or you do not have an MPS-enabled device on this machine.")
    
    mydevice = torch.device("cpu")

else:
    mydevice = torch.device("mps")

# UNET declaration for input of 256x256x1 and output of 256x256x2
# 
# Generated by Copilot, verified by Medium article (Its the same) + Adrian inspection step by step
# checked it works for classification
# TODO: Generalise number of channels and so

class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()
        
        # Contracting path
        self.enc1 = self.conv_block(1, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        self.enc5 = self.conv_block(512, 1024) # Bottleneck / latent space
        
        # Expansive path
        self.upconv4 = self.upconv_block(1024, 512)
        self.dec4 = self.conv_block(1024, 512)
        self.upconv3 = self.upconv_block(512, 256)
        self.dec3 = self.conv_block(512, 256)
        self.upconv2 = self.upconv_block(256, 128)
        self.dec2 = self.conv_block(256, 128)
        self.upconv1 = self.upconv_block(128, 64)
        self.dec1 = self.conv_block(128, 64)
        
        # Output layer. Linear activation makes it Regression, not Classification
        self.out_conv = nn.Conv2d(64, 2, kernel_size=1)
    
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1,device=mydevice),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1,device=mydevice),
            nn.ReLU(inplace=True)
        )
    
    def upconv_block(self, in_channels, out_channels):
        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
    
    def forward(self, x):
        # Contracting path
        enc1 = self.enc1(x)
        enc2 = self.enc2(F.max_pool2d(enc1, 2))
        enc3 = self.enc3(F.max_pool2d(enc2, 2))
        enc4 = self.enc4(F.max_pool2d(enc3, 2))
        enc5 = self.enc5(F.max_pool2d(enc4, 2))
        
        # Expansive path
        dec4 = self.upconv4(enc5)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.dec1(dec1)
        
        out = self.out_conv(dec1)
        return out
    
# Dataload helper function + Data normalization as norm_data = (data-mean)/std
# TODO: parameters: file_dir, number_files,... or authomatise it
def MyDataLoader():
  start_time = time.time()

  input_var_names = ['lon', 'lat',
                    'eastward_model_wind', 'northward_model_wind', 'model_speed', 'model_dir',
                    'msl', 'air_temperature', 'q', 'sst', 'uo', 'vo']
  output_var_names = ['u','v']

  train_input_folder =  "/Volumes/SSD Adrian/TFM/adrian_tfm/ASCAT_l3_collocations/2020/train/"
  loader_input_var_names = ['eastward_model_wind', 'northward_model_wind', 'model_speed', 'model_dir', 
                              'msl', 'air_temperature', 'q', 'sst', 'uo', 'vo']

  input_data = []
  ground_truth = []

  for day in range(1,10):
      all_data = []
      train_input_file = train_input_folder+"ascata_2020010"+str(day)+"_l3_asc.nc"
      f = netDataset(train_input_file)
      
      # Creating 2D variables from 1D data 
      lon = f.variables['lon'].__array__() #0-360 degrees = 2880 points
      lat = f.variables['lat'].__array__() #0-180 degrees = 1440 points
      lons, lats = np.meshgrid(lon, lat)
      all_data.append(lons)
      all_data.append(lats)

      for var_name in loader_input_var_names:
          var_data = f.variables[var_name].__array__()[0]
          all_data.append(var_data)

      # Input data load (X) 
      input_masked_data = np.ma.MaskedArray(all_data)
      #input_masked_data = np.transpose(input_masked_data,(1,2,0)) # Putting the "channels" in the last dimension
      
      # 256x256 piece of image we agreed
      X = input_masked_data[:,864:1120,2568:2824]

      input_data.append(X)
      
      # Ground truth (y)
      u = f.variables['eastward_wind'].__array__()[0]
      v = f.variables['northward_wind'].__array__()[0]
      u_model = f.variables['eastward_model_wind'].__array__()[0]
      v_model = f.variables['northward_model_wind'].__array__()[0]
      f.close()
      targets = np.ma.MaskedArray([u - u_model, v - v_model])
      #targets = np.transpose(targets,(1,2,0))
      y = targets[:,864:1120,2568:2824]
      ground_truth.append(y)

      print(train_input_file," loaded succesfully")

  # let's free some RAM
  del all_data, lon, lat, var_name, input_masked_data, X, u, v, u_model, v_model, targets, y

  gc.collect() # Forcing garbage collection i.e. free RAM references from del listed variables

  end_time = time.time()
  elapsed_time = end_time - start_time

  memory_size = sys.getsizeof(input_data) + sys.getsizeof(ground_truth) # Bytes
  memory_size = memory_size 

  print("Dataload took ",elapsed_time," seconds")
  print("Dataset has ",memory_size,"B allocated in RAM")

  return input_data, ground_truth

# Function to normalise data
# Adrian's Note: np.ma.mean works flawlessly! Test it in EDA, maybe the differences in mean
# are beause of the used method (This one seems to works perfectly)
def MyNorm(BatchedData,NumberExamples):
   for example in range(0,NumberExamples):
    for variable in range(0,len(BatchedData[example])):
        mean = np.ma.mean(BatchedData[example][variable])
        std = np.ma.std(BatchedData[example][variable])
        BatchedData[example][variable] = (BatchedData[example][variable]-mean)/std


   
